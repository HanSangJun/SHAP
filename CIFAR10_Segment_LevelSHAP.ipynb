{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from scipy.io import loadmat\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.transform import resize\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributed GPU\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Num devices: %d\" % strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed precision\n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(\"x_train shape :\", x_train.shape)\n",
    "print(\"x_test shape :\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"y_train shape :\", y_train.shape)\n",
    "print(\"y_test shape :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label smoothing\n",
    "factor = 0.1\n",
    "y_train = (1 - factor) * y_train + (factor / num_classes)\n",
    "\n",
    "print(\"y_train shape :\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few examples\n",
    "plt.figure(figsize=(5, 5))\n",
    "for plot_idx in range(9):\n",
    "    idx = np.random.randint(x_train.shape[0])\n",
    "    plt.subplot(3, 3, plot_idx+1)\n",
    "    plt.imshow(x_train[idx], interpolation=\"quadric\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "print(\"x_train shape :\", x_train.shape)\n",
    "print(\"x_test shape :\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50\n",
    "with strategy.scope():\n",
    "    resnet50 = ResNet50(include_top=False, weights=\"imagenet\")\n",
    "\n",
    "    inputs = layers.Input(shape=(x_train.shape[1], x_train.shape[2], 3))\n",
    "    x = layers.experimental.preprocessing.Resizing(224, 224)(inputs)\n",
    "    x = resnet50(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x, training=True)\n",
    "\n",
    "    x = layers.Dense(num_classes)(x)\n",
    "    outputs = layers.Activation(\"softmax\", dtype=\"float32\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_decay(epoch):\n",
    "    init_lr = 1e-3\n",
    "    drop_rate = 0.85\n",
    "    drop_epochs = 5\n",
    "    \n",
    "    # learning rate decay\n",
    "    lr = init_lr * np.power(drop_rate, np.floor((1+epoch)/drop_epochs))\n",
    "    \n",
    "    if (1 + epoch) % drop_epochs == 0: \n",
    "        print('learning rate is decayed to %f' % lr)\n",
    "        \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 70\n",
    "batch_size = 256\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_decay)\n",
    "file_format = \"./model/model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=file_format, \n",
    "                             monitor=\"val_accuracy\",\n",
    "                             save_best_only=True,\n",
    "                             mode=\"max\")\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, validation_split=0.1, \n",
    "                    callbacks=[lr_scheduler, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Train\", \"Val\"])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Train\", \"Val\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./model/model-21-0.94.hdf5\")\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test loss :\", score[0])\n",
    "print(\"Test accuracy :\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import felzenszwalb\n",
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x_test[29]\n",
    "\n",
    "segments_fz = felzenszwalb(img, scale=100, sigma=0.5, min_size=50)\n",
    "print(\"Felzenszwalb number of segments:\", len(np.unique(segments_fz)))\n",
    "\n",
    "plt.imshow(mark_boundaries(img, segments_fz))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP for Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_label = [\"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \n",
    "                \"Deer\", \"Dog\", \"Frog\", \"Horse\", \"Ship\", \"Truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 3001\n",
    "\n",
    "idx = 256\n",
    "data = x_test[:idx]\n",
    "label = y_test[:idx]\n",
    "mask_value = np.mean(x_train[:1000])\n",
    "\n",
    "count = np.zeros(data.shape[:-1])\n",
    "importance = np.zeros(data.shape[:-1])\n",
    "\n",
    "level = 1\n",
    "level_set_prob = [0.5] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_segment(img, level):\n",
    "    img_with_f = np.copy(img)\n",
    "    img_without_f = np.copy(img)\n",
    "    \n",
    "    level_range = np.arange(level)\n",
    "    scale_list = [50, 100, 150, 250, 500, 1200]\n",
    "    \n",
    "    for i in range(img.shape[0]):\n",
    "        # segment\n",
    "        scale = np.random.choice(scale_list, 1)[0]\n",
    "        segments = felzenszwalb(img[i], scale=scale, sigma=0.5, min_size=20)\n",
    "        \n",
    "        # get the number of ones\n",
    "        feat_dim = np.unique(segments).shape[0]\n",
    "        level_set = np.concatenate([level_range, feat_dim-level_range-1])\n",
    "        num_ones = np.random.choice(level_set, 1, p=level_set_prob)[0]\n",
    "        \n",
    "        # get mask cluster - without replacement\n",
    "        mask_cluster = np.random.choice(np.arange(feat_dim), replace=False, size=feat_dim-num_ones)\n",
    "        active_cluster = np.random.choice(mask_cluster, 1)\n",
    "        \n",
    "        # get mask cluster from segments\n",
    "        for j in mask_cluster:\n",
    "            row, col = np.where(segments == j)\n",
    "            \n",
    "            if j != active_cluster[0]:\n",
    "                img_with_f[i, row, col] = mask_value    \n",
    "            img_without_f[i, row, col] = mask_value\n",
    "            \n",
    "    return img_with_f, img_without_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine m\n",
    "start_time = time()\n",
    "\n",
    "for i in range(0, repeat):\n",
    "    data_with_f, data_without_f = batch_segment(data, level)\n",
    "\n",
    "    # get softmax value\n",
    "    pred_with_f = np.max(model.predict(data_with_f), axis=1)\n",
    "    pred_without_f = np.max(model.predict(data_without_f), axis=1)\n",
    "\n",
    "    # marginal Shapely value\n",
    "    diff = pred_with_f - pred_without_f\n",
    "    f_idx = np.where(data_with_f[:, :, :, 0] != data_without_f[:, :, :, 0])\n",
    "    \n",
    "    # get importance\n",
    "    for batch_idx in range(data.shape[0]):\n",
    "        idx_in_batch = np.where(f_idx[0] == batch_idx)[0]\n",
    "        row = f_idx[1][idx_in_batch]\n",
    "        col = f_idx[2][idx_in_batch]\n",
    "        \n",
    "        importance[batch_idx, row, col] += diff[batch_idx]\n",
    "        count[batch_idx, row, col] += 1\n",
    "    \n",
    "    if i % 1000 == 0 and i > 0: # verbose\n",
    "        print(\"For %dth iter, it takes %0.3f sec\" % (i, time()-start_time))\n",
    "        \n",
    "        for data_num in range(0, data.shape[0]):\n",
    "            img_name = \"./images/fig_segment_iter\" + str(i) + \"_data\" + str(data_num) + \".png\"\n",
    "            heatmap = np.divide(importance[data_num], count[data_num])\n",
    "            pred = model.predict(np.expand_dims(data[data_num], axis=0))\n",
    "            \n",
    "            true_obj = idx_to_label[int(np.argmax(label[data_num]))]\n",
    "            pred_obj = idx_to_label[int(np.argmax(pred, axis=1))]\n",
    "            \n",
    "            ax1 = plt.subplot(1, 2, 1)\n",
    "            im1 = plt.imshow(data[data_num], interpolation=\"quadric\")\n",
    "            plt.title(\"True: %s, Pred: %s\" % (true_obj, pred_obj))\n",
    "            \n",
    "            ax2 = plt.subplot(1, 2, 2)\n",
    "            im2 = plt.imshow(heatmap, cmap=\"coolwarm\")\n",
    "            plt.colorbar(im2, fraction=0.046, pad=0.04)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(img_name, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        start_time = time()\n",
    "    \n",
    "importance = np.divide(importance, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
